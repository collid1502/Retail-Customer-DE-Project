# Creating an AWS Data Engineering Pipeline for a Retail Org

### Foreword<br>
This will be an example project created by myself, in order to utilise a combination of different tools and services, to showcase how they may be used within a classic data engineering example; collating retail data and customer records, which are then ETL'd to analytics tables for downstream use / analysis / visualisation.

The proposal of the project is to use data sources gathered from the web, such as Kaggle Datasets, or Fake Data APIs etc. <br>
The project will update over time to shoecase different AWS resources, as well as coding techniques/langauges and other skills like data modelling etc. <br>
Another goal is to also include some IAC (Terraform) where possible, which will manage the cloud resources.<br>
There is a plan for adding a custom data quality module, to showcase a tool for analysts creating SQL based rules and loading to a metadata database, which can then be used in the pipeline(s). <br>
I will also look to explore CI/CD with GitHub actions or a similar tool. <br>
Finally, I will provide some sample analysis using the analytics tables created, likely via a jupyter notebook, or maybe even include a streamlit app or other visualisation tool for displaying some analysis. 

---------

### Project Sections<br>

#### 1. Sourcing the data<br>

##### 1.1 - Finding a Retail Dataset <br>

HOLDER .....
